# Uncertainty estimation for Large Language Models
The aim of this project is to estimate the uncertainty of Large Language Models. To achieve this, we aim to leverage the Adversary Robustness literature. 
Intuitively, adversarial robustness is a method where an attacker rephrase (perturb) the query and prompt a language model multiple times. If the language model is certain about its answer, it shouldnâ€™t change its answer significantly based on the rephrasings. Formally, robustness can be formulated as :

Let $f$ be a binary classifier model (a classifier that defines if the classifier is in class 0 or 1),   
$f : I \rightarrow \{0,1\}$.   
Let $x$ be an input taken in the input space $I$.  
Let $y$ be its true class, i.e. $f(x) = y \in {0,1}$.   
Let $p$ be a distance on the input space $I$.  
Let $B(x,r, p)$ be a neighborhood of $x$, for a given $r$. This represents the distribution of all perturbed $x$.   
Then a model is robust iff :   
$\forall \bar{x} \in B(x,r,p), P(f(\bar{x})=y) \geq \dfrac{1}{2}.$

By extending this framework to a regression setting (for example $I = \mathbb{R}^n$), and by increasing $r$ until the above equation doesn't hold, it may then be possible to estimate the uncertainty of the model as a function of the number of perturbations $r$.   
Therefore, the approch is as follows : 
- For a question, rephrase the question in multiple ways. By extending the robustness framework to a regression setting, find the most certain answer $y$, and a neighborhood such that **most of** the answers to the rephrased questions lay in this neighborhood.
- From the given outputs, estimate the uncertainty of the Language Model.

Due to time constraints, the objectives of this project were to implement the first step. To do this : 
- We use Google T5-11B as our model to be prompted
- We use TriviaQA as the dataset to test our code
- Rephrasings are generated by substituting each word by a synonym, with a probability ```alpha``` of being substituted. Synonyms are generated using 2 methods, using the AlBERT model, or using word embeddings distance.
- To estimate the most probable region of answers, probabilistic methods are used.
  
## Requirements 
A conda environment can be created using ```environment.yml```
## Code structure
The algorithm is split in three parts : 
### Prompt
In the prompting part, perturbations of a question to be used for the following *smooth* and *algorithm* parts are generated. This groups all calls to the Large Language Model in one script, so that perturbations can be generated at once and other less intensive computations can be done later. This is done using ```prompt.py```, with arguments that are detailed running ```./prompt.py --help```
### Smooth
In the smoothing part, two of the three perturbations batches generated by the *prompt* part are used to find the center of a Minimum Enclosing Ball of the answers to the perturbated prompts by the Large Language Model. This is done with the script ```smooth.py```, with arguments that are detailed running ```./smooth.py --help```
### Certify
This part uses the last perturbation batch generated by ```prompt.py``` to find the radius of the aforementionned Minimum Enclosing Ball. This is done with the script ```certify.py```, with arguments that are detailed running ```./certify.py --help```. The output of this script gives the question, the "center" answer, and the certified radius. 

- Some arguments need to be the same for the three scripts. The consistency of the arguments between the three scripts is kept using .json config files, and the execution of the scripts will be halted if an inconsistence is detected. 

### Common
```common.py``` contains some assets that are used by at least two of the three scripts. Worth noting is the function that computes the Word Mover Distance between two sentences.
## Automatic parameter estimation
Calibration of the parameters ```delta```, ```alpha``` is very important. In fact, if the result given by ```compute_rho.py``` is too close to 1, the quantile taken could be then superior to 1, which doesn't make mathematical sense.
To avoid this problem, ```compute_params.py``` contains a function that computes the optimal ```delta``` and ```alpha``` for the least number of prompts (in ```prompt.py```) possible so that ```compute_rho.py``` gives useable results.
Using these parameters, ```auto_*.py``` scripts runs the same aforementionned three steps on the first 10 questions of the dataset.

## Known issues
- Some words are not present in the Word2Vec vocabulary used to compute Word Mover Distance. A potential fix is developped in ```common.py```, using Google T5 embeddings.
- A better way to calibrate parameters can be found
- The used dataset contains questions that have very short and precise answers. Another dataset could be used to give longer and more varied answers.
